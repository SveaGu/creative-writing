import pickle
import networkx as nx
from networkx.algorithms import community
import random

from gensim.models import word2vec
from gensim.test.utils import datapath
from gensim.models import KeyedVectors
import os
import pandas as pd
import numpy as np
import jieba
import matplotlib.pyplot as plt

wv = KeyedVectors.load_word2vec_format(datapath("D:\\…\\baidu_wiki_corpus_300_new_binary.bin"), binary=True)
stopwords = open(os.path.join('D:\\…\\sempsy-1.0\\textprocess\\tests\\simple_stopwords.txt'), encoding ='utf-8').read().split('\n')
model = wv

#workpath switches to "D:\…\sempsy-1.1"
from cluster.tfidf import *
from semdistance.get_vector import *


#import texts
filepath = r"filepath1" #story1,all subjects
filepath = r"filepath2" #story2,all subjects

[corpus,word_tfidf] = word_tfidf_gensim(filepath,'document') #thisone


'''select words of top X% tfidf value'''

selected_word_tfidf=[]

for i in word_tfidf:
    #print(type(i.values()))
    
    sorted_i = sorted(i.items(),key=lambda item:item[1],reverse=True)
    #extract tfidf values
    x = [a[1] for a in sorted_i]
    
    #top X%:100-X
    n = np.percentile(x,80,axis=0)
    
    #tuple to list
    list_sorted_i = []
    for tran in sorted_i:
        tran = list(tran)
        list_sorted_i.append(tran)
    
    #number of top X% words
    all_tfidf = np.array(list_sorted_i)[:,1]
    number = len([i for i in all_tfidf.flatten().astype('float') if i > float(n)])

    top_i = sorted_i[:number]
    
    #list to dict
    top_i_dict ={}
    for i in top_i:
        top_i_dict[i[0]] = i[1]

    selected_word_tfidf.append(top_i_dict)


'''transform data to edges and links'''
#Get words in the wv corpus (mentioned above), and word vectors of these words

word_vector_list = []
words_list = []
for dict in selected_word_tfidf:
    words_list_sub = list(dict.keys())
    
    vector_list = []
    words = []
    for w in words_list_sub:
        if w in model:
            vector_list.append(model[w])
            words.append(w)
        else:
            print(w)
            
    #word_vector_sub = {words_list_sub :vector_list[i] for i in range(len(vector_list))}
    word_vector_list.append(vector_list)
    words_list.append(words)


'''________________________________________________________________'''

#construct graphs
sub_G=[]
corr_matrix = []
for i in range(len(words_list)):
    res = np.corrcoef(word_vector_list[i])
    
    
    edge_lists = []
    df = pd.DataFrame(res)
    
    edge_name = words_list[i]
    df.index = df.columns = edge_name


    #mask coefficent that is less than 0.05 and 1 (the diagonal)
    tmp_df = df.mask(np.triu(np.ones(df.shape)).astype(bool) | (df < 0.2))
    
    edge_lists = tmp_df.stack().reset_index().apply(tuple, axis=1).values
    
    
    G = nx.Graph()
    G.add_weighted_edges_from(edge_lists)
    
    sub_G.append(G)
    corr_matrix.append(tmp_df)

##Information transfer efficiency
sub_cluster=[]
sub_ASPL=[]
#sub_SW=[] #sigma＞1,small world
sub_modularity=[]
sub_global=[] #global efficiency

for G in sub_G:
    sub_cluster.append(nx.average_clustering(G))
    sub_ASPL.append(nx.average_shortest_path_length(G))
    
    comm=nx.community.greedy_modularity_communities(G)
    sub_modularity.append(nx.community.modularity(G,comm))
    
    sub_global.append(nx.global_efficiency(G))
    
    sub_SW.append(nx.sigma(G))
